{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Pipeline\n",
    "\n",
    "This notebook documents the process of pushing data to a queue through which we write to a shared database with one writer for all streams."
   ],
   "id": "beca76a5ab185bd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data gets pushed from all sources to a single queue.",
   "id": "37ec184d5f6c25d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use the following functions from our database.utils.py file:",
   "id": "5daee53489b8a636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import redis\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "r   = redis.from_url(os.getenv(\"REDIS_URL\"))\n",
    "PRICE_STREAM  = \"prices_stream\"\n",
    "NEWS_STREAM = \"news_stream\"\n",
    "\n",
    "async def publish_price(ticker, price, quantity, timestamp):\n",
    "    await r.xadd(PRICE_STREAM, {\n",
    "        \"ticker\": ticker,\n",
    "        \"price\": float(price),\n",
    "        \"quantity\": float(quantity),\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "\n",
    "async def publish_news(article_id, title, timestamp, tickers):\n",
    "    await r.xadd(NEWS_STREAM, {\n",
    "        \"table\" : \"article\",\n",
    "        \"article_id\": article_id,\n",
    "        \"title\": title,\n",
    "        \"timestamp\": timestamp.isoformat()\n",
    "    })\n",
    "\n",
    "    for ticker in tickers:\n",
    "        await r.xadd(NEWS_STREAM, {\n",
    "            \"table\" : \"mention\",\n",
    "            \"article_id\": article_id,\n",
    "            \"ticker\": ticker\n",
    "        })"
   ],
   "id": "83b83ae1b29ec32f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With this we are able to populate the tables in the database with live data. Right now they are pushed to the redis queue. All streams unify into one single redis stream that then gets written to the database in batches.",
   "id": "90cf57c2d0f5651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import duckdb\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "DB_PATH = 'path_to_database'\n",
    "con = duckdb.connect(DB_PATH)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "async def process_prices(messages):\n",
    "    for _, m in messages:\n",
    "        print(m)\n",
    "\n",
    "    rows = {(\n",
    "        m[\"ticker\"], float(m[\"price\"]), float(m[\"quantity\"]), m[\"timestamp\"]\n",
    "    ) for _, m in messages}\n",
    "    rows = list(rows)\n",
    "    print(rows)\n",
    "    print(type(rows))\n",
    "\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO prices (ticker, price, quantity, timestamp) VALUES (?, ?, ? ,?)\n",
    "        ON CONFLICT DO NOTHING\n",
    "    \"\"\", rows)\n",
    "\n",
    "async def process_news(messages):\n",
    "    articles, mentions = [], []\n",
    "    for _, m in messages:\n",
    "        if m['table'] == 'article':\n",
    "            sentiment = get_sentiment(m['title'])\n",
    "            articles.append((m['article_id'], m['title'], m['timestamp'], sentiment))\n",
    "        else:\n",
    "            mentions.append((m['article_id'], m['ticker']))\n",
    "    if not articles and not mentions:\n",
    "        return\n",
    "\n",
    "    con.execute(\"BEGIN\")\n",
    "    if articles:\n",
    "        con.executemany(\"\"\"\n",
    "            INSERT INTO news_articles (article_id, title, timestamp, sentiment) VALUES (?, ?, ?, ?)\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\", articles)\n",
    "    if mentions:\n",
    "        con.executemany(\"\"\"\n",
    "            INSERT INTO ticker_mentions (article_id, ticker) VALUES (?, ?)\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\", mentions)\n",
    "    con.execute(\"COMMIT\")"
   ],
   "id": "473cd4281690c4f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use the vader sentiment library to help analyze the news articles and give it a sentiment score. Then process_news and process_prices populates the tables in the database. This is done in batches to increase efficiency.",
   "id": "352df1431a93d964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import asyncio\n",
    "\n",
    "async def snapshot_to_parquet():\n",
    "    while True:\n",
    "        try:\n",
    "            con.execute(\"COPY (SELECT * FROM prices) TO 'data/prices.parquet' (FORMAT PARQUET)\")\n",
    "            con.execute(\"COPY (SELECT * FROM news_articles) TO 'data/news_articles.parquet' (FORMAT PARQUET)\")\n",
    "            con.execute(\"COPY (SELECT * FROM ticker_mentions) TO 'data/ticker_mentions.parquet' (FORMAT PARQUET)\")\n",
    "        except Exception as e:\n",
    "            print(\"Parquet export error:\", e)\n",
    "        await asyncio.sleep(1)  # export every second\n"
   ],
   "id": "c556b6d1ead95f21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This method allows us to export data from the database into a parquet file which allows for faster reads, amd better data compression. Our main reason behind this is that our connection to the database is always open in the writer.py file so we needed another way of reading data live.",
   "id": "802bc2cf6579ae70"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
